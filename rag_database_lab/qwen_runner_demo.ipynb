{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ced6228",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation with VLMs Demo\n",
    "\n",
    "Notebook to compare different architectures of VLMs capabilities for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c2936",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Set working directory (optional) and verify script path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine notebook and repo root paths\n",
    "from pathlib import Path\n",
    "notebook_dir = Path.cwd()\n",
    "print('Notebook directory:', notebook_dir)\n",
    "# Ascend one level to get desired repository root (/.../vlm_laboratories/vlm_laboratories)\n",
    "repo_root = notebook_dir.parent\n",
    "print('Derived repo root:', repo_root)\n",
    "# Environment \n",
    "script_path = repo_root / 'prompt_engineering_lab' / 'live_vlm_test' / 'vlm_runners.py'\n",
    "print('Phi3.5 runner script exists:', script_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940f779",
   "metadata": {},
   "source": [
    "## 2. Import VLMRunnerPhi\n",
    "Dynamically load the class from runners script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VLMRunnerPhi (headless safe)\n",
    "import os, sys, importlib\n",
    "os.environ['PYGLET_HEADLESS'] = 'true'\n",
    "os.environ.pop('DISPLAY', None)\n",
    "for m in list(sys.modules):\n",
    "    if m.startswith('pyglet'):\n",
    "        del sys.modules[m]\n",
    "importlib.invalidate_caches()\n",
    "import pyglet\n",
    "pyglet.options['headless'] = True\n",
    "pyglet.options['shadow_window'] = False\n",
    "module_dir = repo_root / 'prompt_engineering_lab' / 'live_vlm_test'\n",
    "if str(module_dir) not in sys.path:\n",
    "    sys.path.append(str(module_dir))\n",
    "from vlm_runners import VLMRunnerPhi\n",
    "print('Imported VLMRunnerPhi from', module_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db9313",
   "metadata": {},
   "source": [
    "## 3. Smoke Test Generation\n",
    "Instantiate the runner and generate text from a synthetic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test (simple description)\n",
    "from PIL import Image\n",
    "import torch, os, gc\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:64'\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "example_img_dir = repo_root / 'rag_database_lab' / 'example_images'\n",
    "example_img_path = sorted(example_img_dir.glob('*.png'))[0] if list(example_img_dir.glob('*.png')) else None\n",
    "print('Using example image:', example_img_path)\n",
    "try:\n",
    "    runner = VLMRunnerPhi(torch.device('cpu'))  # keep CPU for portability\n",
    "    if example_img_path:\n",
    "        test_img = Image.open(example_img_path).convert('RGB')\n",
    "    else:\n",
    "        test_img = Image.new('RGB', (224,224), color=(100,120,140))\n",
    "    out = runner.generate(test_img, 'Describe the scene briefly.')\n",
    "    print(out[:500])\n",
    "except Exception as e:\n",
    "    print('Generation failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08978588",
   "metadata": {},
   "source": [
    "## 4. Load Traffic Rules\n",
    "Load and normalize rules from `rules.json` for retrieval tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1552205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rules\n",
    "import json, pandas as pd\n",
    "rules_path = repo_root / 'rag_database_lab' / 'rules.json'\n",
    "with open(rules_path, 'r', encoding='utf-8') as f:\n",
    "    raw_rules = json.load(f)\n",
    "rules = []\n",
    "for r in raw_rules:\n",
    "    if isinstance(r, dict) and 'id' in r and 'rule_text' in r:\n",
    "        rules.append({\n",
    "            'id': r['id'],\n",
    "            'text': ' '.join(r['rule_text'].strip().split())\n",
    "        })\n",
    "print(f'Loaded {len(rules)} rules')\n",
    "pd.DataFrame(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e0186",
   "metadata": {},
   "source": [
    "## 5. Select an Example Image\n",
    "Choose one of the example traffic scene images to evaluate rules. You can re-run the next cell to sample another random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "images_dir = repo_root / 'rag_database_lab' / 'example_images'\n",
    "all_images = sorted([p for p in images_dir.glob('*.png')])\n",
    "if not all_images:\n",
    "    raise FileNotFoundError(f'No PNG images found in {images_dir}')\n",
    "\n",
    "selected_image = random.choice(all_images)\n",
    "print(f'Selected image: {selected_image.name}')\n",
    "\n",
    "# Display the chosen image\n",
    "try:\n",
    "    from PIL import Image\n",
    "    img = Image.open(selected_image)\n",
    "    display(img)\n",
    "except Exception as e:\n",
    "    print('Could not open image with PIL:', e)\n",
    "\n",
    "selected_image_path = str(selected_image)  # keep a str path for model calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c005d6",
   "metadata": {},
   "source": [
    "## 6. Iterative Rule Evaluation\n",
    "We will ask Phi about each rule separately to obtain an applicability score. Prompt schema aims for a JSON with fields: rule_id, score (0-100), reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statistics import mean\n",
    "\n",
    "# Ensure we have the runner instance; reuse existing if present\n",
    "try:\n",
    "    runner  # noqa: F821\n",
    "except NameError:\n",
    "    runner = None\n",
    "\n",
    "if runner is None:\n",
    "    # Fallback import if not already done\n",
    "    from vlm_runners import VLMRunnerPhi # type: ignore\n",
    "    runner = VLMRunnerPhi(torch.device(\"cpu\"))\n",
    "\n",
    "iterative_results = []\n",
    "\n",
    "# Load base_instruction from a .txt file if available\n",
    "prompt_txt_path = repo_root / 'rag_database_lab' / 'base_instruction.txt'\n",
    "if prompt_txt_path.exists():\n",
    "    with open(prompt_txt_path, 'r', encoding='utf-8') as f:\n",
    "        base_instruction = f.read().strip()\n",
    "\n",
    "start_time = time.time()\n",
    "image = Image.open(selected_image_path).convert('RGB')\n",
    "\n",
    "\n",
    "for r in rules:  # iterate all loaded rules\n",
    "    rule_id = r[\"id\"]\n",
    "    rule_text = r[\"text\"]\n",
    "    prompt = (\n",
    "        f\"{base_instruction}\\nRule ID: {rule_id}, rule {rule_text}\\n\" \n",
    "        f\"Analyze applicability. Return JSON only.\"\n",
    "    )\n",
    "\n",
    "    raw = runner.generate(image, prompt)\n",
    "    iterative_results.append(raw)\n",
    "    print(raw)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce0494",
   "metadata": {},
   "source": [
    "## 7. Batch Evaluation (All Rules at Once)\n",
    "We provide all rules in a single prompt requesting a JSON array ranking applicability for the displayed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build batch prompt\n",
    "rules_block_lines = [f\"{r['id']}. {r['text']}\" for r in rules]\n",
    "rules_block = \"\\n\".join(rules_block_lines)\n",
    "\n",
    "# Load batch_instruction from a .txt file if available\n",
    "batch_txt_path = repo_root / 'rag_database_lab' / 'batch_instruction.txt'\n",
    "if batch_txt_path.exists():\n",
    "    with open(batch_txt_path, 'r', encoding='utf-8') as f:\n",
    "        batch_instruction = f.read().strip()\n",
    "else:\n",
    "    raise FileNotFoundError(f'Batch instruction file not found: {batch_txt_path}')\n",
    "\n",
    "batch_prompt = (\n",
    "    f\"{batch_instruction}\\nCandidate Rules:\\n{rules_block}\\n\"\n",
    "    f\"Analyze the image and produce the output in the correct form.\"\n",
    ")\n",
    "\n",
    "batch_raw = runner.generate(image ,batch_prompt)\n",
    "print(batch_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefab43",
   "metadata": {},
   "source": [
    "## 8. Compare Iterative vs Batch Results\n",
    "We align scores by rule_id, compute a simple correlation, and inspect agreement among top rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair comparison logic for batch prompt that returns ONLY an ordered JSON array of rule_id strings.\n",
    "# Batch output (batch_raw) contains strictly: [\"RULE_ID_1\", \"RULE_ID_2\", ..., \"RULE_ID_N\"]. No scores.\n",
    "# Iterative results still include per-rule JSON objects with score + reason.\n",
    "\n",
    "import re, json\n",
    "from math import sqrt\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def _extract_first_json_obj(text: str):\n",
    "    m = re.search(r'\\{[^{}]*\\}', text, re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    snippet = m.group(0)\n",
    "    cleaned = snippet.replace(\"'\", '\"')\n",
    "    cleaned = re.sub(r',\\s*}', '}', cleaned)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _norm_score(v):\n",
    "    try:\n",
    "        x = float(v)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "# Parse iterative (raw strings list aligned with rules order)\n",
    "parsed_iter = []\n",
    "for idx, raw in enumerate(iterative_results):\n",
    "    rule_id = rules[idx]['id']\n",
    "    rule_text = rules[idx]['text']\n",
    "    obj = _extract_first_json_obj(raw) or {}\n",
    "    score = _norm_score(obj.get('score', 0))\n",
    "    reason = str(obj.get('reason', ''))[:300]\n",
    "    parsed_iter.append({\n",
    "        'rule_id': rule_id,\n",
    "        'rule_text': rule_text,\n",
    "        'score': score,\n",
    "        'reason': reason,\n",
    "        'raw': raw[:350]\n",
    "    })\n",
    "\n",
    "print(f\"Parsed {len(parsed_iter)} iterative results with scores.\")\n",
    "\n",
    "# Parse batch: ordered array of rule_ids only\n",
    "batch_order = []\n",
    "batch_array_match = re.search(r'\\[.*?\\]', batch_raw, re.DOTALL)\n",
    "if batch_array_match:\n",
    "    snippet = batch_array_match.group(0)\n",
    "    cleaned = snippet.replace(\"'\", '\"')\n",
    "    # Remove trailing commas before closing bracket\n",
    "    cleaned = re.sub(r',\\s*]', ']', cleaned)\n",
    "    try:\n",
    "        data = json.loads(cleaned)\n",
    "        if isinstance(data, list):\n",
    "            batch_order = [str(x) for x in data]\n",
    "    except Exception:\n",
    "        batch_order = []\n",
    "\n",
    "# Validate batch_order: ensure all rule_ids appear exactly once\n",
    "expected_ids = [str(r['id']) for r in rules]\n",
    "if set(batch_order) != set(expected_ids) or len(batch_order) != len(expected_ids):\n",
    "    print(\"WARNING: Batch output missing or duplicating rule_ids. Attempting recovery.\")\n",
    "    # Fallback: if some IDs embedded individually, collect them preserving first occurrence order\n",
    "    found_ids = []\n",
    "    for rid in expected_ids:\n",
    "        if rid in batch_raw and rid not in found_ids:\n",
    "            found_ids.append(rid)\n",
    "    if set(found_ids) == set(expected_ids):\n",
    "        batch_order = found_ids\n",
    "\n",
    "print(f\"Parsed batch ordering of {len(batch_order)} rule_ids.\")\n",
    "\n",
    "# Build rank mapping (1-based). Earlier position => higher applicability.\n",
    "batch_rank = {rid: i + 1 for i, rid in enumerate(batch_order)}\n",
    "N = len(batch_order) if batch_order else 0\n",
    "\n",
    "# Correlate iterative scores with batch ranking.\n",
    "# Transform batch rank to a 'rank_score' where higher means more applicable: rank_score = N + 1 - rank\n",
    "pairs = []\n",
    "for entry in parsed_iter:\n",
    "    rid = str(entry['rule_id'])\n",
    "    if rid in batch_rank:\n",
    "        iter_score = entry['score']\n",
    "        rank_score = N + 1 - batch_rank[rid]\n",
    "        pairs.append((iter_score, rank_score))\n",
    "\n",
    "if pairs:\n",
    "    xs = [p[0] for p in pairs]\n",
    "    ys = [p[1] for p in pairs]\n",
    "    mean_x = sum(xs)/len(xs)\n",
    "    mean_y = sum(ys)/len(ys)\n",
    "    num = sum((x-mean_x)*(y-mean_y) for x,y in pairs)\n",
    "    den = (sum((x-mean_x)**2 for x in xs) * sum((y-mean_y)**2 for y in ys)) ** 0.5\n",
    "    corr = num/den if den else 0.0\n",
    "else:\n",
    "    corr = 0.0\n",
    "\n",
    "print(f\"Correlation (iterative score vs batch rank position): {corr:.3f} over {len(pairs)} rules\")\n",
    "\n",
    "# Top-k iterative vs top-k batch (first k in ordering)\n",
    "K = 5\n",
    "iter_top_ids = [str(r['rule_id']) for r in sorted(parsed_iter, key=lambda x: x['score'], reverse=True)[:K]]\n",
    "batch_top_ids = batch_order[:K]\n",
    "overlap = sorted(set(iter_top_ids) & set(batch_top_ids))\n",
    "print(f\"Top-{K} overlap count: {len(overlap)} | IDs: {overlap}\")\n",
    "\n",
    "# Display alignment for overlapping rules\n",
    "for rid in overlap:\n",
    "    it_score = next(r['score'] for r in parsed_iter if str(r['rule_id']) == rid)\n",
    "    brank = batch_rank.get(rid)\n",
    "    text = next((r['text'] for r in rules if str(r['id']) == rid), '')\n",
    "    print(f\"Rule {rid}: iter_score={it_score:.2f} batch_rank={brank} | {text[:90]}\")\n",
    "\n",
    "# Build comparison summary structure\n",
    "comparison_summary = {\n",
    "    'correlation_iterative_score_vs_batch_rank': corr,\n",
    "    'iter_top': iter_top_ids,\n",
    "    'batch_top': batch_top_ids,\n",
    "    'top_overlap': overlap,\n",
    "    'iter_count': len(parsed_iter),\n",
    "    'batch_count': N\n",
    "}\n",
    "comparison_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00c6e2",
   "metadata": {},
   "source": [
    "## 9. CLIP Embedding-Based Rule Retrieval\n",
    "Use a dual-encoder (CLIP) to embed the selected image and all rule texts, rank rules by cosine similarity, and compare with Qwen batch ordering & iterative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35924280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP retriever and compute rule/image embeddings\n",
    "from clip_rule_retrieval import CLIPRuleImageRetriever, compute_metrics\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "clip_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "retriever = CLIPRuleImageRetriever(device=clip_device)\n",
    "print(f'Loaded CLIP model on {clip_device}')\n",
    "\n",
    "# Ensure image object exists\n",
    "if 'image' not in globals():\n",
    "    image = Image.open(selected_image_path).convert('RGB')\n",
    "\n",
    "# Embed rules (list of dicts with id, text)\n",
    "rule_embs, rule_ids, rule_texts = retriever.embed_rules(rules)\n",
    "print(f'Embedded {len(rule_ids)} rules -> tensor shape {tuple(rule_embs.shape)}')\n",
    "\n",
    "# Embed image\n",
    "img_emb = retriever.embed_image(image)\n",
    "print('Image embedding shape:', tuple(img_emb.shape))\n",
    "\n",
    "# Rank rules by similarity\n",
    "top_k = min(10, len(rule_ids))\n",
    "ranking = retriever.rank_rules(img_emb, rule_embs, rule_ids, rule_texts, top_k=top_k)\n",
    "\n",
    "print('\\nTop CLIP rule ranking:')\n",
    "for r in ranking:\n",
    "    print(f\"rule_id={r['rule_id']} sim={r['similarity']:.4f} | {r['rule_text'][:90]}\")\n",
    "\n",
    "# Prepare metrics inputs\n",
    "clip_rank_ids = [str(r['rule_id']) for r in ranking]  # truncated list\n",
    "full_clip_rank_ids = [str(r['rule_id']) for r in retriever.rank_rules(img_emb, rule_embs, rule_ids, rule_texts, top_k=None)]\n",
    "iter_scores_map = {str(e['rule_id']): e['score'] for e in parsed_iter}\n",
    "\n",
    "# Compute metrics using full ordering vs batch_order (if available)\n",
    "metrics = compute_metrics(full_clip_rank_ids, batch_order, iter_scores_map, k=5)\n",
    "print('\\nExample metrics summary:')\n",
    "for k,v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f'{k}: {v:.4f}')\n",
    "    else:\n",
    "        print(f'{k}: {v}')\n",
    "\n",
    "clip_metrics = metrics  # expose for later cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
