{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967ccf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enhanced configuration for multiview fusion + output saving\n",
    "MODEL_KEY = 'qwen25'\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''\n",
    "OUTPUT_DIR = 'outputs'\n",
    "from utils import ensure_dir\n",
    "ensure_dir(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ccff5",
   "metadata": {},
   "source": [
    "# Experiment 2: Multiview Fusion (Camera + Map)\n",
    "\n",
    "Compare action suggestions when the model receives both front camera and bird's-eye map images. Measure delta vs single-view baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f6b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY = 'qwen25'\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''\n",
    "GROUND_TRUTH = ['LEFT','STOP','SLOW','LEFT','STOP','SLOW']  # example; adjust or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc53ce6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from utils import list_frame_pairs, build_multiview_prompt, generate_action, parse_action_json\n",
    "pairs = list_frame_pairs(folder_name='example_edge_samples')\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5eba6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/newvens/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e9666be0f1457fb7f04f2ffe8dc298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'action': 'LEFT',\n",
       "  'rationale': 'The road curves to the left, indicating that turning left is the correct direction.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The robot is on a straight path with no obstacles or turns ahead.'},\n",
       " {'action': 'LEFT',\n",
       "  'rationale': 'The robot is on a curved road, and the yellow dashed lines indicate a left turn. The robot should follow the curve.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The road is clear ahead, and there are no obstacles or traffic signals indicating a need to change direction.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': 'The road is blocked by traffic cones, indicating that it is not safe to proceed forward.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The rubber duck is in the center of the road, indicating no immediate obstacles or need to change direction.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = build_multiview_prompt(EXTRA_PROMPT)\n",
    "outputs = []\n",
    "parsed = []\n",
    "for cam_path, map_path in pairs:\n",
    "    cam = Image.open(cam_path).convert('RGB')\n",
    "    map_img = Image.open(map_path).convert('RGB')\n",
    "    out = generate_action(MODEL_KEY, [cam, map_img], prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    outputs.append(out)\n",
    "    parsed.append(parse_action_json(out) or {})\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ed7cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LEFT', 'FORWARD', 'LEFT', 'FORWARD', 'STOP', 'FORWARD']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [p.get('action','?') for p in parsed]\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9564a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Persist actions for multiview fusion\n",
    "from utils import save_text, parse_action_json\n",
    "if 'parsed' in globals():\n",
    "    import json, os\n",
    "    actions = [p.get('action','?') for p in parsed]\n",
    "    summary = {'actions': actions}\n",
    "    save_text(os.path.join(OUTPUT_DIR,'experiment2_multiview_summary.json'), json.dumps(summary, indent=2))\n",
    "    summary\n",
    "else:\n",
    "    print('Run generation cells first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
