{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a13a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configuration and ground truth loading / output saving setup\n",
    "MODEL_KEY = 'qwen25'\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''\n",
    "OUTPUT_DIR = 'outputs'\n",
    "GROUND_TRUTH_PATH = 'example_edge_samples/edge_samples_ground_truth.json'\n",
    "from utils import ensure_dir\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "GROUND_TRUTH = []\n",
    "import os, json\n",
    "if os.path.isfile(GROUND_TRUTH_PATH):\n",
    "    gt_map = json.load(open(GROUND_TRUTH_PATH,'r',encoding='utf-8'))\n",
    "else:\n",
    "    gt_map = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef2966",
   "metadata": {},
   "source": [
    "# Experiment 1: Single-View Baseline Action Prediction\n",
    "\n",
    "Goal: Evaluate baseline performance of a VLM when given ONLY the front camera image for each frame. We request a JSON action + rationale.\n",
    "Metrics: action accuracy (if ground truth available), JSON validity, latency per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a8ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_KEY = 'qwen25'  # choose from utils.VLM_MODELS keys\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''  # optional additional constraint\n",
    "GROUND_TRUTH = ['LEFT','STOP','SLOW','LEFT','STOP','SLOW']  # example; adjust or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a1adda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame01_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame01_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame02_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame02_map.png')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time\n",
    "from PIL import Image\n",
    "from utils import list_frame_pairs, build_single_view_prompt, generate_action, parse_action_json, action_accuracy\n",
    "pairs = list_frame_pairs(folder_name='example_edge_samples')\n",
    "pairs[:2]  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0fea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/newvens/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510816245c72482fa08d8014f6cbc8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'action': 'FORWARD',\n",
       "  'rationale': 'The road is clear ahead, and there are no obstacles or signs indicating a need to change direction.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The robot is on a straight road with no obstacles or traffic, so it should continue forward.'},\n",
       " {'action': 'LEFT',\n",
       "  'rationale': 'The road curves to the left, indicating the need to turn left.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The road is clear ahead, and there are no obstacles or signs indicating a need to change direction.'},\n",
       " {'action': 'FORWARD',\n",
       "  'rationale': 'The road is clear ahead, and there are no obstacles or traffic signals indicating otherwise.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': 'There is an obstacle (a rubber duck) on the road, which could cause a collision.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = build_single_view_prompt(EXTRA_PROMPT)\n",
    "results = []\n",
    "parsed = []\n",
    "latencies = []\n",
    "for idx,(cam_path,map_path) in enumerate(pairs):\n",
    "    img = Image.open(cam_path).convert('RGB')\n",
    "    start = time.time()\n",
    "    out = generate_action(MODEL_KEY, [img], prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    latencies.append(time.time()-start)\n",
    "    results.append(out)\n",
    "    pj = parse_action_json(out) or {}\n",
    "    parsed.append(pj)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf85ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.0, 'json_validity': 1.0, 'avg_latency_s': 36.94840463002523}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FORWARD', 'FORWARD', 'LEFT', 'FORWARD', 'FORWARD', 'STOP']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "actions = [p.get('action','?') for p in parsed]\n",
    "accuracy = action_accuracy(parsed, GROUND_TRUTH) if GROUND_TRUTH else None\n",
    "validity = sum(1 for p in parsed if 'action' in p)/len(parsed) if parsed else 0\n",
    "print({'accuracy': accuracy, 'json_validity': validity, 'avg_latency_s': sum(latencies)/len(latencies)})\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7eb7bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save metrics summary at end\n",
    "from utils import action_accuracy, parse_action_json, save_text\n",
    "# If earlier cells ran, we expect variables: parsed, latencies, pairs\n",
    "if 'parsed' in globals():\n",
    "    actions = [p.get('action','?') for p in parsed]\n",
    "    acc = action_accuracy(parsed, GROUND_TRUTH) if GROUND_TRUTH else None\n",
    "    validity = sum(1 for p in parsed if 'action' in p)/len(parsed) if parsed else 0\n",
    "    summary = {\n",
    "        'actions': actions,\n",
    "        'accuracy': acc,\n",
    "        'json_validity': validity,\n",
    "        'avg_latency_s': sum(latencies)/len(latencies) if latencies else None\n",
    "    }\n",
    "    import json, os\n",
    "    save_text(os.path.join(OUTPUT_DIR,'experiment1_summary.json'), json.dumps(summary, indent=2))\n",
    "    summary\n",
    "else:\n",
    "    print('Run generation cells first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
