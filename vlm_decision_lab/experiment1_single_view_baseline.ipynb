{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configuration and ground truth loading / output saving setup\n",
    "MODEL_KEY = 'qwen25'\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''\n",
    "OUTPUT_DIR = 'outputs'\n",
    "GROUND_TRUTH_PATH = 'example_edge_samples/edge_samples_ground_truth.json'\n",
    "from utils import ensure_dir\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "GROUND_TRUTH = []\n",
    "import os, json\n",
    "if os.path.isfile(GROUND_TRUTH_PATH):\n",
    "    gt_map = json.load(open(GROUND_TRUTH_PATH,'r',encoding='utf-8'))\n",
    "else:\n",
    "    gt_map = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef2966",
   "metadata": {},
   "source": [
    "# Experiment 1: Single-View Baseline Action Prediction\n",
    "\n",
    "Goal: Evaluate baseline performance of a VLM when given ONLY the front camera image for each frame. We request a JSON action + rationale.\n",
    "Metrics: action accuracy (if ground truth available), JSON validity, latency per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_KEY = 'qwen25'  # choose from utils.VLM_MODELS keys\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''  # optional additional constraint\n",
    "GROUND_TRUTH = ['LEFT','STOP','SLOW','LEFT','STOP','SLOW']  # example; adjust or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from PIL import Image\n",
    "from utils import list_frame_pairs, build_single_view_prompt, generate_action, parse_action_json, action_accuracy\n",
    "pairs = list_frame_pairs(folder_name='example_edge_samples')\n",
    "pairs[:2]  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_single_view_prompt(EXTRA_PROMPT)\n",
    "results = []\n",
    "parsed = []\n",
    "latencies = []\n",
    "for idx,(cam_path,map_path) in enumerate(pairs):\n",
    "    img = Image.open(cam_path).convert('RGB')\n",
    "    start = time.time()\n",
    "    out = generate_action(MODEL_KEY, [img], prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    latencies.append(time.time()-start)\n",
    "    results.append(out)\n",
    "    pj = parse_action_json(out) or {}\n",
    "    parsed.append(pj)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "actions = [p.get('action','?') for p in parsed]\n",
    "accuracy = action_accuracy(parsed, GROUND_TRUTH) if GROUND_TRUTH else None\n",
    "validity = sum(1 for p in parsed if 'action' in p)/len(parsed) if parsed else 0\n",
    "print({'accuracy': accuracy, 'json_validity': validity, 'avg_latency_s': sum(latencies)/len(latencies)})\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7eb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics summary at end\n",
    "from utils import action_accuracy, parse_action_json, save_text\n",
    "# If earlier cells ran, we expect variables: parsed, latencies, pairs\n",
    "if 'parsed' in globals():\n",
    "    actions = [p.get('action','?') for p in parsed]\n",
    "    acc = action_accuracy(parsed, GROUND_TRUTH) if GROUND_TRUTH else None\n",
    "    validity = sum(1 for p in parsed if 'action' in p)/len(parsed) if parsed else 0\n",
    "    summary = {\n",
    "        'actions': actions,\n",
    "        'accuracy': acc,\n",
    "        'json_validity': validity,\n",
    "        'avg_latency_s': sum(latencies)/len(latencies) if latencies else None\n",
    "    }\n",
    "    import json, os\n",
    "    save_text(os.path.join(OUTPUT_DIR,'experiment1_summary.json'), json.dumps(summary, indent=2))\n",
    "    summary\n",
    "else:\n",
    "    print('Run generation cells first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
