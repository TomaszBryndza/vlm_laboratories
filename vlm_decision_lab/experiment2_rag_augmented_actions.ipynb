{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af9b77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enhanced configuration for RAG augmented experiment\n",
    "MODEL_KEY = 'qwen25'\n",
    "TOP_K_RULES = 3\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''\n",
    "OUTPUT_DIR = 'outputs'\n",
    "from utils import ensure_dir\n",
    "ensure_dir(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a43c4",
   "metadata": {},
   "source": [
    "# Experiment 3: RAG-Augmented Action Decisions\n",
    "\n",
    "Inject top-k retrieved driving rules (CLIP similarity) before asking for action decision. Compare outputs vs non-RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b67606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY = 'qwen25'\n",
    "TOP_K_RULES = 3\n",
    "MAX_NEW_TOKENS = 128\n",
    "EXTRA_PROMPT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a9e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame01_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame01_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame02_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame02_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame03_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame03_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame04_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame04_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame05_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame05_map.png'),\n",
       " ('/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame06_cam.png',\n",
       "  '/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/vlm_decision_lab/example_edge_samples/frame06_map.png')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from utils import list_frame_pairs, build_single_view_prompt, build_rag_prompt, generate_action, retrieve_rules_for_image, parse_action_json\n",
    "pairs = list_frame_pairs(folder_name='example_edge_samples')\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840a5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/o0i3z3/thesis/vlm_laboratories/vlm_laboratories/newvens/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b88d6c4bc24d6fa57b373481bd12d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_prompt = build_single_view_prompt(EXTRA_PROMPT)\n",
    "rag_outputs = []\n",
    "baseline_outputs = []\n",
    "parsed_rag = []\n",
    "parsed_base = []\n",
    "for cam_path, map_path in pairs:\n",
    "    cam = Image.open(cam_path).convert('RGB')\n",
    "    ranked = retrieve_rules_for_image(cam, top_k=TOP_K_RULES)\n",
    "    rag_prompt = build_rag_prompt(baseline_prompt, ranked, max_rules=TOP_K_RULES)\n",
    "    base_out = generate_action(MODEL_KEY, [cam], baseline_prompt)\n",
    "    rag_out = generate_action(MODEL_KEY, [cam], rag_prompt)\n",
    "    baseline_outputs.append(base_out)\n",
    "    rag_outputs.append(rag_out)\n",
    "    parsed_base.append(parse_action_json(base_out) or {})\n",
    "    parsed_rag.append(parse_action_json(rag_out) or {})\n",
    "len(rag_outputs), len(baseline_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973b7d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SLOW',\n",
       "  'rationale': 'The road is curving, indicating the need to adapt speed to maintain control and visibility.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': 'The rubber duck obstructs the lane, making it unsafe to proceed forward.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': \"The robot is approaching a curve on the road, and it's important to slow down before curves to ensure safe navigation.\"},\n",
       " {'action': 'SLOW',\n",
       "  'rationale': 'The road has a curve ahead, requiring reduced speed for safe navigation.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': 'There are traffic cones blocking the lane, indicating that it is unsafe to proceed forward.'},\n",
       " {'action': 'STOP',\n",
       "  'rationale': 'A yellow duck is on the road, which requires yielding to pedestrians or animals.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578517a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_changes_due_to_rag': 4}\n"
     ]
    }
   ],
   "source": [
    "# Simple comparison: count action changes introduced by rules\n",
    "baseline_actions = [p.get('action','?') for p in parsed_base]\n",
    "rag_actions = [p.get('action','?') for p in parsed_rag]\n",
    "changes = sum(1 for b,r in zip(baseline_actions, rag_actions) if b != r)\n",
    "print({'action_changes_due_to_rag': changes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d4af9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Persist baseline vs RAG comparison summary\n",
    "from utils import save_text\n",
    "if 'parsed_rag' in globals():\n",
    "    import json, os\n",
    "    base_actions = [p.get('action','?') for p in parsed_base]\n",
    "    rag_actions = [p.get('action','?') for p in parsed_rag]\n",
    "    changes = sum(1 for b,r in zip(base_actions, rag_actions) if b != r)\n",
    "    summary = {\n",
    "        'baseline_actions': base_actions,\n",
    "        'rag_actions': rag_actions,\n",
    "        'action_changes_due_to_rag': changes\n",
    "    }\n",
    "    save_text(os.path.join(OUTPUT_DIR,'experiment3_rag_summary.json'), json.dumps(summary, indent=2))\n",
    "    summary\n",
    "else:\n",
    "    print('Run generation cell first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
